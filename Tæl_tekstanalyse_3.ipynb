{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tekstanalyse 3: SpaCy og data frames\n",
    "***\n",
    "***\n",
    "Keywords: `SpaCy`, `data frames`, `parts of speech tagging`, `POS`, `sentiment analyse`,\n",
    "\n",
    "Nye Python-udtryk:  `.getcwd()`, `.glob`, `.pos_`, `.join()`, `afinn.score()`, `Counter()`\n",
    "***\n",
    "***\n",
    "I det følgende skal vi arbejde videre med NLP-pakken `SpaCy`. Vi har allerede kigget på mulighederne for at lave **named entity recognition**, og vi skal nu forsøge at kombinere **Parts of Speech Tagging** og **Sentiment analysis** med henblik på at undersøge, i hvilket omfang de seneste ti års statsministernytårstaler har været overvejende positive eller negative.\n",
    "\n",
    "Nogle af elementerne vil være repetition af elementer fra tidligere notebooks.\n",
    "\n",
    "Hvis der er kode sekvenser eller udtryk I ikke forstår, er det altid en god idé at bruge Google. Det kan give svar på det meste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Forberedelse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "Som altid begynder vi med at importere de nødvendige `libraries`. Udover de sædvanlige moduler skal vi også bruge `glob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                       # os tillader os bl.a. at finde filplaceringer på computeren\n",
    "import numpy as np              # Numpy leverer noget af matematikken, der ligger under Pandas \n",
    "import pandas as pd             # Pandas tillader os at importere, oprette og manipulere data frames\n",
    "import matplotlib.pyplot as plt # Importerer underbiblioteket pyplot fra pakken matplotlib\n",
    "from nltk.text import Text      # nltk indholder mange forskellige funktioner, der kan bruges til tekstanalyse\n",
    "import glob                     # glob-modulet hjælper med at finde specifikke filnavne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herudover skal vi også importere `SpaCy`. Når vi bruger spacy, skal vi udover at importere modulet, også loade en sprogmodel. Der er tre modeller at vælge mellem: `da_core_news_sm`, `da_core_news_md`, `da_core_news_lg`, en lille (sm), en mellem (md) og en stor (lg). Størrelsen angiver, hvor stort et korpus modellen er blevet trænet på. \n",
    "\n",
    "**Importér** SpaCy og **load** den store danske sprog-model. Den kører lidt langsommere end de andre, men er til gengæld mere præcis. Skal man arbejde med meget store tekstmængder, kan det være en idé at bruge en mindre model. Det er en afvejning af forbrug af regnekraft og tid mod præcision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy          \n",
    "nlp = spacy.load(\"da_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import og klargøring af tekster\n",
    "I denne notebook skal vi arbejde med de seneste 10 års nytårstaler. To af Mette Frederiksen, fire af Lars Løkke Rasmussen og fire af Helle Thorning Schmidt. \n",
    "\n",
    "**Placér** mappen med de downloadede `.txt`-filer i den mappe i den mappe, hvori i har gemt jeres script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find den aktuelle sti\n",
    "Hvis I ikke kender den aktuelle sti, kan nedenstående kommando bruges. Kommandoen forudsætter, at `os`-modulet er **importeret** (hvilket vi gjorde ovenfor):\n",
    "\n",
    "`directory_path = os.getcwd()`<br>\n",
    "`print(directory_path)`\n",
    "\n",
    "Kommandoen `.getcwd()`returnerer den mappe I aktuelt arbejder i, ofte omtalt som *current working directory* (cwd).\n",
    "\n",
    "I kan **copy/paste** den printede fil-sti, når vi skal om lidt skal importere talerne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virker på pc\n",
    "Denne sekvens er magen til den vi brugte sidst. Punktummet i `'.\\Taler'` peger på den aktuelle mappe (cwd).\n",
    "\n",
    "Hvis du arbejder på en pc, virker denne sekvens fint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taler_x = [] # opretter tom liste\n",
    "\n",
    "for fil in os.scandir(r'.\\Taler'): # for-loop\n",
    "    with open (fil, encoding = \"utf8\") as f: # context manager\n",
    "        taler_x.append(f.read().replace(\"\\n\",\" \").replace(\"*\",\" \")) # tilføj renset tekst til liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virker både på mac og pc\n",
    "Hvis du i stedet arbejder på en mac, skal du bruge denne sekvens (pc-brugere kan også bruge denne sekvens). Med `glob`-modulet, som vi importerede indledningsvis, har vi mulighed for at matche filnavne. Asterisken i `\\Taler\\*.txt` betyder **alle filer** med endenlsen `.txt`. På denne måde åbner vi kun de tekst-filer, der ligger i mappen.\n",
    "\n",
    "De problemer, vi tidligere er stødt på i forbindelse med afvikling af import-loop på mac-computere, skyldes, at Mac OS (styresystemet) gemmer skjulte filer i alle mapper. Filerne har praktiske funktioner og kan **ikke** ses i **pathfinder**. De skaber normalt ikke problemer, bortset fra når vi skal importere filer med Python. Ved at bruge `glob`-modulet bliver det muligt kun at importere de filer, der har endelsen `.txt`, hvilket sikrer, at de skjulte filer sorteres fra.\n",
    "\n",
    "**Bemærk** at I enten kan vælge at copy/paste hele stien eller kan vælge `.\\Taler\\`, hvor punktummet angiver på på den aktuelle mappe (cwd).\n",
    "\n",
    "**Husk** at vende skråstregerne i fil-stierne, hvis I arbejder på en mac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taler = []\n",
    "#for roman in glob.glob(r'C:\\Users\\au100440\\Desktop\\1_Tæl din tekst\\Jupyter_scripts\\Taler\\*.txt'): \n",
    "for roman in glob.glob(r'.\\Taler\\*.txt'): # henter alle filer i mappen 'Taler', der har fileendelsen .txt    \n",
    "    with open(roman, \"r\", encoding=\"utf-8\") as f: \n",
    "         taler.append(f.read().replace(\"\\n\",\" \").replace(\"*\",\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at sikre os, at filerne er importeret, som de skal, vi printe de første tegn i hver tekst.\n",
    "\n",
    "Med dette `for loop` beder vi om at få printet de første tyve tegn fra hvert element (teksterne er stadig en lang `string`) fra listen 'Taler'.\n",
    "\n",
    "`for t in taler:`<br>\n",
    "&emsp;`print(t[:20])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organisér listen i en bestemt rækkefølge\n",
    "Hvis talerne ikke er blevet indlæst i den ønskede rækkefølge, kan I **reorganisere** elementerne på listen på følgende måde.\n",
    "\n",
    "`nyRække = [5, 3, 2, 0, 1, 4, 6, 9, 8, 7]`<br>\n",
    "`taler_reorg = [taler[t] for t in nyRække]`\n",
    "\n",
    "Rækkefølgen i `nyRække` er et eksempel. Kig på listen ovenfor, og giv HTS_2012 nummer **1**, HTS_2013 nummer **2** osv. \n",
    "\n",
    "Den anden kodelinje laver en ny liste `taler_reorg`, hvor den som input tager tallene (i den rækkefølge de står) fra `nyRække` og bruger dem som indeks for udvælgelse af elementer fra listen `taler`. **Læses** noget i retning af 'Tag første tal fra `ny_række` (dvs 5); udvælg elementet med index 5 fra listen `taler`; tilføj elementet til listen `taler_reorg`. Gentag sekvense med andet tal fra `nyRække` (dvs 3) osv.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print** den nye liste for at tjekke rækkefølgen. Brug samme loop som ovenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis I har reorganiseret talerne, så brug lsiten `taler_reorg` ellers den oprindelige liste `taler`. Nedenfor fortsætter jeg med `taler`, men I kan bare æbdre det til `taler_reorg`, eller hvad I nu kan finde på at kalde listen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titler\n",
    "Næste skridt er at lave en liste med titler, som vi kan bruge, når vi skal lave vores `data frame`.\n",
    "\n",
    "Ved hjælp af `list comprehension` laver vi en liste, hvor vi splitter hver tale o ord, dvs. ved blanktegn, og derefter tilføjer det første element fra hver liste (index 0), dvs titlen, til en ny liste som vi kalder \"titler\":\n",
    "\n",
    "`titler = [t.split(\" \")[0] for t in taler]`\n",
    "\n",
    "**Kør** kodesekvensen i feltet nedenfor. **Diskutér** hvad de enkelte dele betyder. **Inspicér** indholdet af variablen `titler` og tjek at alt ser ud som det skal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tekst uden titel\n",
    "Herefter fjerner vi tilen fra alle taler, så vi får en ren tekst. Da alle taler begynder med 'Godaften' gøres dette lette ved at splitte teksten ved 'godaften' og derefter tilføje anden del (indeks 1) til listen `taler_ren`.\n",
    "\n",
    "`taler_ren = [t.split(\"Godaften\")[1] for t in taler]`\n",
    "\n",
    "**Kør** kodesekvensen i feltet nedenfor. **Diskutér** hvad de enkelte dele betyder. **Inspicér** indholdet af variablen `taler_ren` og tjek at alt ser ud som det skal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Lav en `data frame`\n",
    "Vi kan nu lave en `data frame` ud af kapitlerne på listen med følgende kommando:\n",
    "\n",
    "`df_nytår = pd.DataFrame({\"ID\": titler, \"Tekst\": taler_ren})`\n",
    "\n",
    "**Indtast** koden i feltet nedenfor. **Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. **Inspicér** jeress dataframe og tjek, at alt er i orden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi har allerede importeret `SpaCy`, load'et den store danske sprogmodel og gemt den unde navnet `nlp`, så SpaCy-modulet er klar til brug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tilføj kolonne med nlp-objekter\n",
    "Med `.apply()` kan vi anvende `nlp()` på alle teksterne i vores `data frame`. Vi gemmer i samme omgang nlp-objekterne i en ny kolonne med navnet 'nlp_Tekst'.\n",
    "\n",
    "`df_nytår[\"nlp_Tekst\"] = df_nytår.Tekst.apply(nlp)`\n",
    "\n",
    "**Indtast** koden i feltet nedenfor. **Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. \n",
    "\n",
    "**Inspicér** jeress dataframe og tjek, at alt er i orden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sentiment analyse med `afinn`\n",
    "**Sentiment analyse** dækker over forskellige teknikker til at beskrive om en tekst er udtryk for **positive** eller **negative** holdninger. Dette kan gøres på forskellige måder. Med `afinn`-modulet, der også virker på dansk, tildeles udvalgte ord, typisk navneord og adjektiver, en score mellem 5 (mest positiv) og -5 (mest negativ). Tallene lægges sammen til en samlet score for teksten.\n",
    "\n",
    "For at kunne bruge `afinn`-modulet, skal i åbne terminalen og indtaste følgende kommando efterfulgt af Enter:\n",
    "\n",
    "`pip install afinn`\n",
    "\n",
    "Mere om sentiment analyse:\n",
    "https://www.freecodecamp.org/news/what-is-sentiment-analysis-a-complete-guide-to-for-beginners/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definér funktion\n",
    "Som udgangspunkt kan vi godt køre en sentiment analyse på hele teksten. Vi kan dog også vælge at lave analyse på udvalgte ordklasser - dette kan varieres på forskellige måder afhængigt af formålet.\n",
    "\n",
    "Nedenfor **defineres** en funktion, der hver tekst udtrækker en liste af **adjektiver** og **substantiver**. Dette gøres ved hjælp af `.pos_`-kommandoen fra SpaCy-modluet.\n",
    "\n",
    "Funktionen kan let modificeres, ved at tilføje eller fjerne tags.\n",
    "\n",
    "**Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ord(txt):\n",
    "    x = []\n",
    "    y = list(txt.sents)\n",
    "    for s in y:                   \n",
    "        for w in s:               \n",
    "            if w.pos_ == \"ADJ\" or w.pos_ == \"NOUN\":\n",
    "                x.append(w.text)\n",
    "    return sorted(set(x))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med `.apply()` kan vi let tilføje en ny kolonne med de nye ordlister.\n",
    "\n",
    "`df_nytår[\"Subst_adj\"] = df_nytår.nlp_Tekst.apply(find_ord)`\n",
    "\n",
    "**Indtast** koden i feltet nedenfor. **Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`afinn`-modulet tager strings som input, så inden vi kan lave analysen, skal vi have transformeret ordlisten til en samlet streng. Dette gøres med `.join()`-kommandoen.\n",
    "\n",
    "Da vi skal gøre det for alle ti taler, er det nemmest at lave en funktion. `\" \"` angiver, at der skal indsættes et blanktegn mellem hvert ord, der joines - altså den modsatte proces at `split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streng(lst):\n",
    "    return \" \".join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan nu anvende funktionen på kolonne med ordlisterne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nytår[\"Subst_adj_streng\"] = df_nytår.Subst_adj.apply(streng)\n",
    "df_nytår.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herefter **importerer** vi `afinn`-modulet og loader den danske ordliste. Fremgangsmåden minder om den måder vi arbejder med SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn(language='da')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan nu til føje en ny kolonne med en sentiment score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nytår[\"Sentiment_score\"] = df_nytår.Subst_adj_streng.apply(afinn.score)\n",
    "df_nytår"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi vil se nærmere på, hvordan scoren er sammensat, kan vi score de enkelte ord i en tale.\n",
    "\n",
    "**Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in taler[0].split(\" \"):\n",
    "    print(w, afinn.score(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En anden måde at får overblik er ved at bruge funktionen `counter()`, der kan tælle antallet af forekomster af de enkelte ord.\n",
    "\n",
    "**Læs** kodesekvensen og **diskutér** hvad de forskellige dele betyder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(taler[0].split(\" \"))\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan plotte vores sentiment score som et stolpediagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "\n",
    "plt.title(\"Nytårstalernes sentiment score\")\n",
    "\n",
    "plt.bar(df_nytår.ID,df_nytår.Sentiment_score)\n",
    "\n",
    "plt.ylabel(\"Sentiment score\")\n",
    "plt.xlabel(\"Nytårstale\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"nytårs_score.pdf\", bbox_inches = \"tight\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gem data frame som csv-fil\n",
    "For at gemme data-framen som csv-fil skal i bruge følgende kommando. Filnavnet er naturligvis valgfrit:\n",
    "\n",
    "`df_nytår.to_csv('nytårstaler_sentiment.csv', index=False)`\n",
    "\n",
    "\n",
    "Tilføjelsen `index=False` sikrer, at data-framen ikke gemmes med det automatisk generede index (første kolonne: 0, 1, 2, osv.). Der genereres som default et nyt index, hver gang data-framen åbnes, og vi ville derfor have først to, så tre indekser osv., hvis vi gemmer indekse hver gang.\n",
    "\n",
    "Filen gemmes som default i den fil-mappe, hvori scriptet er gemt.\n",
    "\n",
    "**Gem**  data-frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ekstraopgave\n",
    "\n",
    "## A) Kvalitativ uddybning\n",
    "**Diskutér** hvordan vi med de ressourcer, vi her har til rådighed, kan få en bedre kvalitativ fortåelse for, hvilket indhold, der ligger bag den rå sentiment score.\n",
    "\n",
    "Prøv at **konstruere** kodeeksempler, der kan understøtte den kvalitative udforskning af, hvad der gemmer sig bag sentiment scoren.\n",
    "\n",
    "## B) Normalisering af score\n",
    "Sentiment scoren er summen af scoren for de enkelte ord i en tekst. Længere tekster vil derfor med meget stor sandsynlighed have en højere score end kortere tekster, alene fordi de er længere. **Diskutér** mulige måder at løse dette problem. Prøv at konstruere kodeeksempler, der giver jer sentimentscorer, der er lettere at sammenligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
